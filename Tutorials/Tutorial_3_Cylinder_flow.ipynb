{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070d2146",
   "metadata": {},
   "source": [
    "# SPICY Tutorial 3\n",
    "\n",
    "In this tutorial, we implement a constrained regression of a 2D velocity field, and we combine the lessons learned in the previous two tutorials to compute pressure fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134e5b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.io\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..' + os.sep + 'spicy_vki' + os.sep + 'spicy')\n",
    "from spicy_class import spicy\n",
    "\n",
    "# This is for plot customization\n",
    "fontsize = 12\n",
    "plt.rc('text', usetex=True)      \n",
    "plt.rc('font', family='serif')\n",
    "plt.rcParams['xtick.labelsize'] = fontsize\n",
    "plt.rcParams['ytick.labelsize'] = fontsize\n",
    "plt.rcParams['axes.labelsize'] = fontsize\n",
    "plt.rcParams['legend.fontsize'] = fontsize\n",
    "plt.rcParams['font.size'] = fontsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8781fc6",
   "metadata": {},
   "source": [
    "This test case is the 2D flow past a cylinder, a typical benchmark test case. The data is taken from https://github.com/Raocp/PINN-laminar-flow. The geometry has a height of $0.41\\,$m, a length of $1.1\\,$m, and the cylinder a radius of $0.02\\,$m placed slightly off the middle of the channel. The solution is provided in the form of a .mat file which is read with Scipy.\n",
    "\n",
    "To prepare the dataset, we remove the points at the inlet and the wall, as these will be introduced as constraints. We then extract the remaining 18755 points for the regression. As for the Oseen vortex, we add some noise to the velocity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed to ensure reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Properties of the domain and flow\n",
    "R = 0.05 # m\n",
    "H = 0.41 # m\n",
    "L = 1.1 # m\n",
    "mu = 2e-2 # Pa s\n",
    "rho = 1 # kg/m^3\n",
    "\n",
    "# Load the matlab data from the ansys solution\n",
    "data = scipy.io.loadmat('FluentSol.mat')\n",
    "# Extract the x, y values\n",
    "X = data['x'].reshape(-1); Y = data['y'].reshape(-1) \n",
    "# Extract the velocities and pressure\n",
    "U = data['vx'].reshape(-1); V = data['vy'].reshape(-1); P = data['p'].reshape(-1)\n",
    "\n",
    "# Here, we remove the points at the inlet and at the wall, as they are given by the constraints\n",
    "inlet_and_wall_remover = np.invert(np.logical_or(np.logical_and(U==0, V==0), X==0))\n",
    "# Remove the points\n",
    "X_p = X[inlet_and_wall_remover]; Y_p = Y[inlet_and_wall_remover]; P_p = P[inlet_and_wall_remover]\n",
    "U_p = U[inlet_and_wall_remover]; V_p = V[inlet_and_wall_remover]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359d3bb",
   "metadata": {},
   "source": [
    "From the remaining points we can choose to sample a random amount of points if we want to go for a smaller test case. In this\n",
    "tutorial, we take a maximum of 18755 points. We will use 80% of these and keep 20% to evaluate the RBF prediction performances in out of sample conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99783318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A testing data, we take the ones that are not in this list.\n",
    "from sklearn.model_selection import train_test_split\n",
    "indices_train, indices_test=train_test_split(np.arange(0,len(X_p),1),test_size=0.2)\n",
    "\n",
    "n_p = len(indices_train) # This is the number of data points that will be used for training\n",
    "\n",
    "# Select the data points that will be used for  training \n",
    "X = X_p[indices_train]; Y = Y_p[indices_train]; P = P_p[indices_train]\n",
    "U = U_p[indices_train]; V = V_p[indices_train]\n",
    "\n",
    "# Add 0.3 noise to the velocity field\n",
    "q = 0.05\n",
    "U_noise = U * (1 + q * np.random.uniform(-1, 1, size = U.shape))\n",
    "V_noise = V * (1 + q * np.random.uniform(-1, 1, size = V.shape))\n",
    "\n",
    "# Let's have a look at the velocity data:\n",
    "fig=plt.figure(11)\n",
    "plt.scatter(X,Y,c=np.sqrt(U**2+V**2))\n",
    "plt.gca().set_aspect(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b24a41a",
   "metadata": {},
   "source": [
    "### Step 1: Define the boundary conditions/constraints\n",
    "We define the BC's below. 50 constraints in each of the five domain boundaries: inlet+ outlet, upper+ lower wall and cylinder wall. We put Dirichlet boundary conditions on all boundaries except the outlet and divergence-free conditions on all of them.\n",
    "\n",
    "At the inlet, we impose the parabolic velocity profile from the CFD data. In all walls we set zero velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff2230",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boundary and Constraint Definitions\n",
    "n_c = 50\n",
    "\n",
    "# Left boundary (index: 1)\n",
    "X_Div1 = np.zeros(n_c)\n",
    "Y_Div1 = np.linspace(0, H, n_c)\n",
    "U_Dir1 = 4*(H-Y_Div1)*Y_Div1/H**2\n",
    "V_Dir1 = np.zeros(X_Div1.shape)\n",
    "# Bottom boundary (index: 2)\n",
    "X_Div2 = np.linspace(0, L, n_c)\n",
    "Y_Div2 = np.zeros(n_c)\n",
    "U_Dir2 = np.zeros(X_Div2.shape)\n",
    "V_Dir2 = np.zeros(X_Div2.shape)\n",
    "# Right boundary (index: 3)\n",
    "X_Div3 = np.ones(n_c-2)*L\n",
    "Y_Div3 = np.linspace(0, H, n_c)[1:-1]\n",
    "# Top boundary (index: 4)\n",
    "X_Div4 = np.linspace(0, L, n_c)\n",
    "Y_Div4 = np.ones(n_c)*H\n",
    "U_Dir4 = np.zeros(X_Div4.shape)\n",
    "V_Dir4 = np.zeros(X_Div4.shape)\n",
    "# Cylinder boundary (index: 4)\n",
    "alphaT = np.linspace(0, 2*np.pi, 20*n_c, endpoint = False)\n",
    "X_Div5 = 0.2+R*np.cos(alphaT)\n",
    "Y_Div5 = 0.2+R*np.sin(alphaT)\n",
    "U_Dir5 = np.zeros(X_Div5.shape)\n",
    "V_Dir5 = np.zeros(X_Div5.shape)\n",
    "\n",
    "# We assemble the velocity constraints for Dirichlet\n",
    "X_Dir = np.concatenate((X_Div1, X_Div2, X_Div4, X_Div5))\n",
    "Y_Dir = np.concatenate((Y_Div1, Y_Div2, Y_Div4, Y_Div5))\n",
    "U_Dir = np.concatenate((U_Dir1, U_Dir2, U_Dir4, U_Dir5))\n",
    "V_Dir = np.concatenate((V_Dir1, V_Dir2, V_Dir4, V_Dir5))\n",
    "# and Divergence-free flow\n",
    "X_Div = np.concatenate((X_Div1, X_Div2, X_Div3, X_Div4, X_Div5))\n",
    "Y_Div = np.concatenate((Y_Div1, Y_Div2, Y_Div3, Y_Div4, Y_Div5))\n",
    "\n",
    "# We remove the duplicates in the Dirchlet \n",
    "_, valid_idcs = np.unique(np.column_stack((X_Div, Y_Div)), return_index = True, axis = 0)\n",
    "X_Div = X_Div[valid_idcs]\n",
    "Y_Div = Y_Div[valid_idcs]\n",
    "DIV = [X_Div, Y_Div]\n",
    "\n",
    "# and Divergence-free conditions\n",
    "_, valid_idcs = np.unique(np.column_stack((X_Dir, Y_Dir)), return_index = True, axis = 0)\n",
    "X_Dir = X_Dir[valid_idcs]\n",
    "Y_Dir = Y_Dir[valid_idcs]\n",
    "U_Dir = U_Dir[valid_idcs]\n",
    "V_Dir = V_Dir[valid_idcs]\n",
    "DIR = [X_Dir, Y_Dir, U_Dir, V_Dir]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487ec207",
   "metadata": {},
   "source": [
    "### Step 2: Clustering with Different Regions\n",
    "\n",
    "In the same way local mesh refinements are often used in CFD, SPICY allows for defining regions of difference collocation density. We can expect velocity gradients to be much larger near the cylinder, somewhere up to x=0.4 (see the previous plot).\n",
    "\n",
    "We will thus define two boxes: one near the cylinder that will have the largest collocation density, another slightly larger having an intermediate density of collocation points. Then, we will also include very large Gaussians in the rest of the domain. First, let's create the boxes using shapely\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bdd817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import geometry\n",
    "\n",
    "# Definition of Box 1\n",
    "p1 = geometry.Point(0.1,0.1); p2 = geometry.Point(0.3,0.1)\n",
    "p3 = geometry.Point(0.3,0.3); p4 = geometry.Point(0.1,0.3)\n",
    "\n",
    "pointList = [p1, p2, p3, p4]\n",
    "poly1 = geometry.Polygon([i for i in pointList])  # BOX one\n",
    "\n",
    "# Definition of Box 2 \n",
    "\n",
    "# Define also a second box\n",
    "p1 = geometry.Point(0.0,0); p2 = geometry.Point(0.5,0)\n",
    "p3 = geometry.Point(0.5,0.45); p4 = geometry.Point(0.0,0.45)\n",
    "\n",
    "pointList = [p1, p2, p3, p4]\n",
    "poly2 = geometry.Polygon([i for i in pointList])\n",
    "\n",
    "# Plot the boxes with the velocity field\n",
    "fig=plt.figure()\n",
    "plt.quiver(X,Y,U,V)\n",
    "plt.gca().set_aspect(1)\n",
    "plt.plot(*poly2.exterior.xy,'b') # in case you want to see them for info\n",
    "plt.plot(*poly1.exterior.xy,'r') # in case you want to see them for info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d21a201",
   "metadata": {},
   "source": [
    "Note that it is not necessary to have the boxes within the domain. The clustering will involve at each level the points (particles) available within each box. We now proceed creating the SPICY object and providing the Areas as shapely polygons.\n",
    "\n",
    "We will use four levels, with n_k=[3,6,50,200], with the areas Areas=[poly1,poly2,[],[]]. This means that the last two clustering levels will act on the entire domain.\n",
    "\n",
    "Then we plot the results at each level.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0f9afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the SPICY object\n",
    "SP_vel = spicy([U_noise,V_noise], [X,Y], basis='gauss')\n",
    "# Clustering \n",
    "SP_vel.clustering([3,6,50,200], Areas=[poly1,poly2,[],[]], r_mM=[0.03,0.6], eps_l=0.87)\n",
    "# Introduce the Constraints\n",
    "SP_vel.vector_constraints(DIR=DIR, DIV=DIV, extra_RBF=True)\n",
    "# Plot the RBF cluster\n",
    "SP_vel.plot_RBFs(l=0)\n",
    "SP_vel.plot_RBFs(l=1)\n",
    "SP_vel.plot_RBFs(l=2)\n",
    "SP_vel.plot_RBFs(l=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02aff67",
   "metadata": {},
   "source": [
    "Note that the cluster closer to the cylinder, with the highest density, hits the lower boundary of allowed RBF radiuses: as a result, SPICY sets all RBFs at the smallest radius allowed. Use the zooming functionality on Spyder to explore the clustering levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a58f1b2",
   "metadata": {},
   "source": [
    "### Step 2: RBF Training and testing\n",
    "\n",
    "We proceed with the assembly of the linear system and the 'training' of the RBF. Note: we will allow a fairly large K_cond!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ac2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembly the system\n",
    "SP_vel.Assembly_Regression(alpha_div=1) \n",
    "# Solve the system\n",
    "SP_vel.Solve(K_cond=1e12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63abbd55",
   "metadata": {},
   "source": [
    "Now we evaluate the accuracy of the analytic model on both the testing data and the training data (those points we removed at the beginning!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56af284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the solution\n",
    "U_calc,V_calc=SP_vel.Get_Sol([X,Y])\n",
    "\n",
    "# Magnitude of the RBF solution\n",
    "U_magn_calc = np.sqrt(U_calc**2 + V_calc**2)\n",
    "# Compute the magnitude of the analytical solution\n",
    "U_magn = np.sqrt(U**2 + V**2)\n",
    "# Compute the error in the magnitude\n",
    "error_magn = np.linalg.norm(U_magn_calc - U_magn) / np.linalg.norm(U_magn)\n",
    "# Error in u\n",
    "error_u = np.linalg.norm(U_calc - U) / np.linalg.norm(U)\n",
    "# Error in v\n",
    "error_v = np.linalg.norm(V_calc - V) / np.linalg.norm(V)\n",
    "\n",
    "print('------- In-sample error results ---------')\n",
    "print('Total velocity error: {0:.3f}%'.format(error_magn*100))\n",
    "print('Velocity error in u:  {0:.3f}%'.format(error_u*100))\n",
    "print('Velocity error in v:  {0:.3f}%'.format(error_v*100))\n",
    "\n",
    "# we now check the out-of sample results.\n",
    "# Get out of sample position\n",
    "U_out=U_p[indices_test];  V_out=V_p[indices_test]\n",
    "X_out=X_p[indices_test];  Y_out=Y_p[indices_test]\n",
    "\n",
    "# Out of sample predictions\n",
    "U_calc_O,V_calc_O=SP_vel.Get_Sol([X_out,Y_out])\n",
    "\n",
    "# Magnitude of the RBF solution\n",
    "U_magn_calc_O = np.sqrt(U_calc_O**2 + V_calc_O**2)\n",
    "# Compute the magnitude of the analytical solution\n",
    "U_magn_O = np.sqrt(U_out**2 + V_out**2)\n",
    "# Compute the error in the magnitude\n",
    "error_magn_O = np.linalg.norm(U_magn_calc_O - U_magn_O)/np.linalg.norm(U_magn_O)\n",
    "# Error in u\n",
    "error_u_O = np.linalg.norm(U_calc_O - U_out) / np.linalg.norm(U_out)\n",
    "# Error in v\n",
    "error_v_O = np.linalg.norm(V_calc_O - V_out) / np.linalg.norm(V_out)\n",
    "\n",
    "\n",
    "print('------- Out of sample error results ---------')\n",
    "print('Total velocity error: {0:.3f}%'.format(error_magn_O*100))\n",
    "print('Velocity error in u:  {0:.3f}%'.format(error_u_O*100))\n",
    "print('Velocity error in v:  {0:.3f}%'.format(error_v_O*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa54931",
   "metadata": {},
   "source": [
    "Note that both the in-sample and the out-of-sample errors are quite low! We plot the results together with the data to see the difference... can you see it :) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ee3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, nrows=3, figsize=(15,6), dpi=100, sharex=True, sharey=True)\n",
    "axes[0,0].scatter(X, Y, c=U_calc, s=10)\n",
    "axes[1,0].scatter(X, Y, c=U, s=10)\n",
    "axes[2,0].scatter(X, Y, c=np.abs(U_calc-U), s=10) \n",
    "\n",
    "axes[0,1].scatter(X, Y, c=V_calc, s=10)\n",
    "axes[1,1].scatter(X, Y, c=V, s=10)\n",
    "axes[2,1].scatter(X, Y, c=np.abs(V_calc-V), s=10)  \n",
    "\n",
    "axes[0,2].scatter(X, Y, c=U_magn_calc, s=10)\n",
    "axes[1,2].scatter(X, Y, c=U_magn, s=10)\n",
    "axes[2,2].scatter(X, Y, c=np.abs(U_magn_calc-U_magn), s=10) \n",
    "\n",
    "\n",
    "axes[0,0].set_ylabel('RBF Regression') \n",
    "axes[1,0].set_ylabel('Ground truth')  \n",
    "axes[2,0].set_ylabel('Absolute difference')  \n",
    "\n",
    "axes[0,0].set_title('$u$') \n",
    "axes[0,1].set_title('$v$')  \n",
    "axes[0,2].set_title('$|\\mathbf{u}-\\mathbf{u}_c|$')      \n",
    "for ax in axes.flatten():\n",
    "    ax.set_aspect(1)      \n",
    "fig.tight_layout()      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ff430",
   "metadata": {},
   "source": [
    "### Step 3: Pressure computation\n",
    "\n",
    "As we now have an analytic expression for the velocity field, we proceed with the pressure integration. We start by defining the boundary conditions/constraints. Here's the list:\n",
    "\n",
    "1. On all patches, we introduce Neumann conditions (these will be computed directly from the velocity field). To impose these we will need to prepare the vector of normals over each of them. These conditions do not require extra measurements.\n",
    "\n",
    "2. On the outlet patch, we introduce Dirichlet boundary conditions. Sufficiently far from the cylinder wake, it is reasonable to assume that the pressure has recovered its free stream condition, which in our case is 0 since the CFD that generated the data works with the gauge pressure.\n",
    "\n",
    "3. We will impose a Dirichlet condition somewhere in the domain (and you are asked to move it around!) This simulates having a static pressure probe somewhere in our wind tunnel.\n",
    "\n",
    "let's get to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d37643",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of constraints on each boundary\n",
    "n_c = 100\n",
    "\n",
    "################################# Define the Location for the Neumann conditions (5 patches) #################\n",
    "# We start with the Neumann conditions\n",
    "# Left boundary\n",
    "X_Pres_N1 = np.zeros(n_c)\n",
    "Y_Pres_N1 = np.linspace(0, H, n_c)\n",
    "# Bottom boundary\n",
    "X_Pres_N2 = np.linspace(0.0,L,n_c)\n",
    "Y_Pres_N2 = np.zeros(n_c)\n",
    "# Top boundary\n",
    "X_Pres_N4 = np.linspace(0, L, n_c)\n",
    "Y_Pres_N4 = np.ones(n_c)*H\n",
    "# Cylinder boundary\n",
    "alpha_P = np.linspace(0, 2*np.pi, n_c, endpoint = False) \n",
    "X_Pres_N5 = 0.2 + R*np.cos(alpha_P)\n",
    "Y_Pres_N5 = 0.2 + R*np.sin(alpha_P)\n",
    "# Assemble the the entire array of Neumann points\n",
    "X_Pres_N=np.hstack((X_Pres_N1, X_Pres_N2, X_Pres_N4, X_Pres_N5))\n",
    "Y_Pres_N=np.hstack((Y_Pres_N1, Y_Pres_N2, Y_Pres_N4, Y_Pres_N5))\n",
    "\n",
    "############################### Compute the normal vectors for all patches requireing N conditions #################\n",
    "\n",
    "# We assemble the normals in the same way\n",
    "# Left boundary\n",
    "n_x_1 = np.ones(X_Pres_N1.shape)*(-1)\n",
    "n_y_1 = np.ones(X_Pres_N1.shape)*0\n",
    "# Bottom boundary\n",
    "n_x_2 = np.ones(X_Pres_N2.shape)*0\n",
    "n_y_2 = np.ones(X_Pres_N2.shape)*(-1)\n",
    "# Top boundary\n",
    "n_x_4 = np.ones(X_Pres_N4.shape)*0\n",
    "n_y_4 = np.ones(X_Pres_N4.shape)*1\n",
    "# Cylinder boundary\n",
    "n_x_5 = np.ones(X_Pres_N5.shape)*(-np.cos(alpha_P))\n",
    "n_y_5 = np.ones(X_Pres_N5.shape)*(-np.sin(alpha_P))\n",
    "# Assemble to obtain the entire array of Neumann normals\n",
    "n_x = np.hstack((n_x_1, n_x_2, n_x_4, n_x_5))\n",
    "n_y = np.hstack((n_y_1, n_y_2, n_y_4, n_y_5))  \n",
    "\n",
    "\n",
    "############### Clean for possible repeaded points (usually along corners ) ############################################\n",
    "_, valid_idcs = np.unique(np.column_stack((X_Pres_N, Y_Pres_N)),\n",
    "                          return_index = True, axis = 0)\n",
    "X_Pres_N = X_Pres_N[valid_idcs]\n",
    "Y_Pres_N = Y_Pres_N[valid_idcs]\n",
    "n_x = n_x[valid_idcs]\n",
    "n_y = n_y[valid_idcs]\n",
    "\n",
    "######################### Define location (and value) for the Dirichlet (D) condition at the outlet ###################\n",
    "\n",
    "X_Pres_D3 = np.ones(n_c-2)*L\n",
    "Y_Pres_D3 = np.linspace(0, H, n_c)[1:-1]\n",
    "\n",
    "#################### Define the location of the pressure probe from which we will take another D condition #################\n",
    "\n",
    "x_loc=0.4; y_loc=0.2    ### PLAY WITH THIS !! # this is the approximate location of the pressure prob.\n",
    "\n",
    "# look for the close point\n",
    "x_err=(X_p-x_loc)**2+(Y_p-y_loc)**2\n",
    "index_probe=np.argmin(x_err)\n",
    "\n",
    "# Define the Dirichlet condition associated to the probe.\n",
    "X_S=X_p[index_probe]\n",
    "Y_S=Y_p[index_probe]\n",
    "P_S=P_p[index_probe]\n",
    "\n",
    "# thus the full set of D conditions is :\n",
    "X_Pres_D = np.append(X_Pres_D3,X_S)\n",
    "Y_Pres_D = np.append(Y_Pres_D3,Y_S)\n",
    "P_Pres_D = np.hstack([np.zeros(X_Pres_D3.shape),P_S])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e867b0",
   "metadata": {},
   "source": [
    "We now assembly the Poisson problem. First compute the source term from the velocity field and the N conditions. We perform a slightly differnt clusterng than before (allowing for larger Gaussians) but we keep the same area partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb5b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we compute the required quantities from the velocity field \n",
    "# (neither of the following 2 steps runs if SP_Vel has not been solved)\n",
    "# 1. Evaluate the source term on the RHS of the Poisson equation\n",
    "source_term = SP_vel.Evaluate_Source_Term(grid=[X,Y], rho=rho)\n",
    "# 2. Evaluate the c_N for the N conditions (see Presentation 1)\n",
    "P_Neu = SP_vel.Get_Pressure_Neumann(grid = [X_Pres_N, Y_Pres_N], \n",
    "                                    normals = [n_x, n_y],\n",
    "                                    rho = rho, mu = mu)\n",
    "\n",
    "# We can now proceed with (1) spicy initialization (2) clustering (3) constraint assingment, (4) System Assembly:\n",
    "\n",
    "# We assemble our Neumann and Dirichlet B.C.\n",
    "NEU_P = [X_Pres_N, Y_Pres_N, n_x, n_y, P_Neu]\n",
    "DIR_P = [X_Pres_D, Y_Pres_D, P_Pres_D]\n",
    "\n",
    "SP_pres = spicy([source_term], [X,Y], basis='gauss')\n",
    "SP_pres.clustering([3,10,50,200,1000], Areas=[poly1,poly2,[],[],[]], r_mM=[0.03,1], eps_l=0.87)\n",
    "\n",
    "# And, we set them\n",
    "SP_pres.scalar_constraints(DIR=DIR_P, \n",
    "                           NEU=NEU_P, \n",
    "                           extra_RBF=True)\n",
    "\n",
    "SP_pres.plot_RBFs(l=4) # Plot the clustering a level \n",
    "\n",
    "SP_pres.Assembly_Poisson() # Assembly the system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb106b3b",
   "metadata": {},
   "source": [
    " We solve the system compare the results on the training and testing locations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd3ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_pres.Solve(K_cond=1e9) # Solve the system\n",
    "\n",
    "# Check the results on the 'training data'\n",
    "P_calc = SP_pres.Get_Sol(grid=[X,Y])\n",
    "# print the pressure error\n",
    "error_p = np.linalg.norm(P_calc-P)/np.linalg.norm(P)\n",
    "print('------- in sample pressure error results ---------')\n",
    "print('Total pressure error: {0:.3f}%'.format(error_p*100))\n",
    "\n",
    "# Check the results on the 'testing data'\n",
    "P_calc_test = SP_pres.Get_Sol(grid=[X_p[indices_test],Y_p[indices_test]])\n",
    "P_test=P_p[indices_test]\n",
    "error_p = np.linalg.norm(P_calc_test-P_test)/np.linalg.norm(P_test)\n",
    "print('------- out of sample pressure error results ---------')\n",
    "print('Total pressure error: {0:.3f}%'.format(error_p*100))\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, dpi=100)\n",
    "axes[0].set_title('Original Pressure')\n",
    "sc=axes[0].scatter(X, Y, c=P)\n",
    "plt.colorbar(sc,ax=axes[0])\n",
    "axes[0].plot(X_S,Y_S,'ro')\n",
    "axes[1].set_title('Reconstructed Pressure')\n",
    "sc2=axes[1].scatter(X, Y, c=P_calc)\n",
    "plt.colorbar(sc2,ax=axes[1])\n",
    "axes[2].set_title('Difference')\n",
    "sc3=axes[2].scatter(X, Y, c=P_calc-P)\n",
    "plt.colorbar(sc3,ax=axes[2])\n",
    "\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_aspect(1)\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb077a73",
   "metadata": {},
   "source": [
    "Both in sample and out of sample error are encouraging. Now try to change the position of the pressure prob (or even remove it) and you will see how important it is: getting pressure from image velocimetry is a difficult task and without the help of some Dirichlet conditions you can't do miracles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2efbe99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
